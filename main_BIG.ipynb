{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of concept for incremental data copy/save with BIG data (V20) on GCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief tutorial shows how to recreate the [animated gif](http://oceanparcels.org/animated-gifs/globcurrent_fullyseeded.gif) showing particles in the Agulhas region south of Africa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing the relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parcels import FieldSet, ParticleSet, JITParticle, AdvectionRK4, ErrorCode\n",
    "from datetime import timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the Globcurrent fields from the `GlobCurrent_example_data` directory (note that unlike in the main Parcels tutorial we don't use a dictionary for the filenames here; as they are the same for all variables, we don't need to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = \"/pw/workflows/ocean_parcels_demo/parcels_examples/GlobCurrent_example_data/20*.nc\"\n",
    "variables = {'U': 'eastward_eulerian_current_velocity',\n",
    "             'V': 'northward_eulerian_current_velocity'}\n",
    "dimensions = {'lat': 'lat',\n",
    "              'lon': 'lon',\n",
    "              'time': 'time'}\n",
    "fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create vectors of Longitude and Latitude starting locations on a regular mesh, and use these to initialise a `ParticleSet` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons, lats = np.meshgrid(range(15, 35), range(-40, -30))\n",
    "pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lons, lat=lats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to advect the particles. However, the Globcurrent data that we loaded in is only for a limited, regional domain and particles might be able to leave this domain. We therefore need to tell Parcels that particles that leave the domain need to be deleted. We do that using a `Recovery Kernel`, which will be invoked when a particle encounters an `ErrorOutOfBounds` error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can advect the particles. Note that we do this inside a `for`-loop, so we can save a plot every six hours (which is the value of `runtime`). See the [plotting tutorial](http://nbviewer.jupyter.org/github/OceanParcels/parcels/blob/master/examples/tutorial_plotting.ipynb) for more information on the `pset.show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: pset, DeleteParticle\n",
    "# Outputs: list of files (each file name is in savefile)\n",
    "for cnt in range(3):\n",
    "    # Set filename for output plot\n",
    "    output_image = 'particles'+str(cnt).zfill(2)\n",
    "    print(output_image)\n",
    "    \n",
    "    # First plot the particles\n",
    "    pset.show(savefile=output_image, field='vector', land=True, vmax=2.0)\n",
    "\n",
    "    # Then advect the particles for 6 hours\n",
    "    pset.execute(AdvectionRK4,\n",
    "                 runtime=timedelta(hours=6),  # runtime controls the interval of the plots\n",
    "                 dt=timedelta(minutes=5),\n",
    "                 recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle})  # the recovery kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now has created 3 plots. Note that the original animated gif contained 20 plots, but to keep running of this notebook fast we have reduced the number here. Of course, it is trivial to increase the number of plots by changing the value in the `range()` in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, you can use [ImageMagick](http://www.imagemagick.org/script/index.php) or an online tool to stitch these individual plots together in an animated gif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the above in parallel on remote, custom workers\n",
    "\n",
    "+ Run OceanParcels with VIKING20 data.\n",
    "+ Incrementally copy over data, calculate trajectories, and save output.\n",
    "+ Each instance of OceanParcels is run via a Docker or Singularity container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parsl\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.data_provider.files import File\n",
    "from path import Path\n",
    "from parsl.configs.local_threads import config\n",
    "from parslpw import pwconfig,pwargs\n",
    "parsl.load(pwconfig)\n",
    "\n",
    "print(\"pwconfig loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def run_ocean_parcels(stdout='ocean_parcels.stdout', \n",
    "                      stderr='ocean_parcels.stderr', inputs=[], outputs=[], year=1959, tstep=1):\n",
    "    \n",
    "    import os\n",
    "    run_script = os.path.basename(inputs[0])\n",
    "    container_run = os.path.basename(inputs[1])\n",
    "    out_file = os.path.splitext(os.path.basename(inputs[0]))[0]+'.gif'\n",
    "    out_dir = os.path.basename(outputs[0])\n",
    "    \n",
    "    run_command = \"/bin/bash \" + container_run + \" python \" + run_script\n",
    "    \n",
    "    tstep2 = tstep + 1\n",
    "    \n",
    "    # The text here is interpreted by Python (hence the %s string substitution\n",
    "    # using strings in the tuple at the end of the long text string) and then\n",
    "    # run as a bash app.\n",
    "    return '''\n",
    "        # Copy over data from cloud bucket (two time steps + grid file)\n",
    "        # TODO: breakout into separate bash_app with future for timing.\n",
    "        # TODO: configure shared data space on worker and container mount wrapper\n",
    "        # TODO: as is, this parsl-task is sandboxed separate from other loop execs\n",
    "        yr=%s\n",
    "        t1=%s\n",
    "        t2=%s\n",
    "        gsutil cp gs://viking20/tsplit_1_VIKING20-K301_5d_${yr}0101_${yr}1231_full_${t1}_grid_U.nc ./t1_U.nc\n",
    "        gsutil cp gs://viking20/tsplit_1_VIKING20-K301_5d_${yr}0101_${yr}1231_full_${t2}_grid_U.nc ./t2_U.nc\n",
    "        gsutil cp gs://viking20/tsplit_1_VIKING20-K301_5d_${yr}0101_${yr}1231_full_${t1}_grid_V.nc ./t1_V.nc\n",
    "        gsutil cp gs://viking20/tsplit_1_VIKING20-K301_5d_${yr}0101_${yr}1231_full_${t2}_grid_V.nc ./t2_V.nc\n",
    "        gsutil cp gs://viking20/mesh_mask.nc ./mesh_mask.nc\n",
    "        \n",
    "        # Run trajectory calculation via container\n",
    "        %s\n",
    "        \n",
    "        # Configure and stage output\n",
    "        outdir=%s\n",
    "        outfile=%s\n",
    "        mkdir -p $outdir\n",
    "        mv movie.gif $outdir/$outfile\n",
    "        \n",
    "        # Clean up to save space for next loop\n",
    "        #rm -f *.nc\n",
    "    ''' % (str(year),str(tstep),str(tstep2),run_command,out_dir,out_file)\n",
    "\n",
    "@bash_app\n",
    "def get_date(stdout='getdate.stdout',\n",
    "            stderr='getdate.stderr', inputs=[], outputs=[]):\n",
    "    \n",
    "    import os\n",
    "    out_file = os.path.splitext(os.path.basename(inputs[0]))[0]+'.log'\n",
    "    out_dir = os.path.basename(outputs[0])\n",
    "    \n",
    "    run_command = \"date > \" + out_dir + \"/\" + out_file\n",
    "    \n",
    "    return '''\n",
    "        mkdir -p %s\n",
    "        %s\n",
    "    ''' % (out_dir, run_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_TESTING = False\n",
    "if LOCAL_TESTING:\n",
    "    import argparse\n",
    "    pwargs = argparse.Namespace()\n",
    "    pwargs.out_dir = '/pw/storage/test-outputs'\n",
    "    pwargs.run_files = '/pw/workflows/ocean_parcels_demo/test_ocean_parcels_1.py---/pw/workflows/ocean_parcels_demo/test_ocean_parcels_2.py'\n",
    "    wrapper = Path(\"/pw/workflows/ocean_parcels_demo/ocean-parcels/wrap_docker_oceanparcels.sh\")\n",
    "else:\n",
    "    if pwargs.container_type == \"True\":\n",
    "        wrapper = Path(\"/pw/workflows/ocean_parcels_demo/ocean-parcels/wrap_docker_oceanparcels.sh\")\n",
    "    else:\n",
    "        wrapper = Path(\"/pw/workflows/ocean_parcels_demo/ocean-parcels/wrap_singularity_oceanparcels.sh\")\n",
    "        \n",
    "run_files = pwargs.run_files.split('---')\n",
    "\n",
    "runs=[]\n",
    "for run_file_name in run_files:\n",
    "    \n",
    "    run_file = Path(run_file_name)\n",
    "    out_dir = Path(pwargs.out_dir)\n",
    "    \n",
    "    r = run_ocean_parcels(inputs=[run_file,wrapper], \n",
    "                          outputs=[out_dir])\n",
    "    \n",
    "    runs.append(r)\n",
    "\n",
    "print(\"Running\",len(runs),\"OceanParcels executions...\")\n",
    "[r.result() for r in runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
