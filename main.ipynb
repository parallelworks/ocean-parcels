{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial showing how to create Parcels in Agulhas animated gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief tutorial shows how to recreate the [animated gif](http://oceanparcels.org/animated-gifs/globcurrent_fullyseeded.gif) showing particles in the Agulhas region south of Africa.  The first half of this demo is based on the work of [Delandmeter and Van Sebille (2019)](https://www.geosci-model-dev.net/12/3571/2019/gmd-12-3571-2019.html) and [Ocean Parcels](https://oceanparcels.org/) and copied from this [Ocean Parcels example](https://github.com/OceanParcels/parcels/blob/master/parcels/examples/tutorial_Agulhasparticles.ipynb).  The second half of this notebook parallelizes this process with the [Parsl parallel Python scripting package](https://parsl-project.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parcels import FieldSet, ParticleSet, JITParticle, AdvectionRK4, ErrorCode\n",
    "from datetime import timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load velocity fields\n",
    "Now load the Globcurrent fields provided with this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = \"/pw/workflows/ocean_parcels_demo/parcels_examples/GlobCurrent_example_data/20*.nc\"\n",
    "variables = {'U': 'eastward_eulerian_current_velocity',\n",
    "             'V': 'northward_eulerian_current_velocity'}\n",
    "dimensions = {'lat': 'lat',\n",
    "              'lon': 'lon',\n",
    "              'time': 'time'}\n",
    "fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesh the grid\n",
    "Now create vectors of Longitude and Latitude starting locations on a regular mesh, and use these to initialise a `ParticleSet` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons, lats = np.meshgrid(range(15, 35), range(-40, -30))\n",
    "pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lons, lat=lats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set boundary conditions for particles\n",
    "Now we want to advect the particles. However, the Globcurrent data that we loaded in is only for a limited, regional domain and particles might be able to leave this domain. We therefore need to tell Parcels that particles that leave the domain need to be deleted. We do that using a `Recovery Kernel`, which will be invoked when a particle encounters an `ErrorOutOfBounds` error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advect particles\n",
    "Note that we advect particles inside a `for`-loop, so we can save a plot every six hours (which is the value of `runtime`). See the [plotting tutorial](http://nbviewer.jupyter.org/github/OceanParcels/parcels/blob/master/examples/tutorial_plotting.ipynb) for more information on the `pset.show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: pset, DeleteParticle\n",
    "# Outputs: list of files (each file name is in savefile)\n",
    "for cnt in range(3):\n",
    "    # Set filename for output plot\n",
    "    output_image = 'particles'+str(cnt).zfill(2)\n",
    "    print(output_image)\n",
    "    \n",
    "    # First plot the particles\n",
    "    pset.show(savefile=output_image, field='vector', land=True, vmax=2.0)\n",
    "\n",
    "    # Then advect the particles for 6 hours\n",
    "    pset.execute(AdvectionRK4,\n",
    "                 runtime=timedelta(hours=6),  # runtime controls the interval of the plots\n",
    "                 dt=timedelta(minutes=5),\n",
    "                 recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle})  # the recovery kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now has created 3 plots. Note that the original animated gif contained 20 plots, but to keep running of this notebook fast we have reduced the number here. One can increase the number of plots by changing the value in the `range()` in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, you can use [ImageMagick](http://www.imagemagick.org/script/index.php) or an online tool to stitch these individual plots together in an animated gif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the above in parallel on remote, custom workers\n",
    "\n",
    "+ Run OceanParcels in 4 separate experiments at the same time.\n",
    "+ Each experiment is defined by a Python/OceanParsels script.\n",
    "+ One experiment per (customizable) worker node.\n",
    "+ Each instance of OceanParcels is run via a Docker container.\n",
    "\n",
    "You can choose to run this part of the notebook in two different ways:\n",
    "1. launched from the OCEAN_PARCELS_DEMO workflow form or\n",
    "2. launched right here from the notebook.\n",
    "In both cases, please ensure that a compute resource is turned on first.  For the second case, a `pw.conf` file (obtained from a job directory from a previous run with the same resource configuration) must be present in the directory that this notebook is running from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parsl\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.data_provider.files import File\n",
    "from path import Path\n",
    "from parsl.configs.local_threads import config\n",
    "from parslpw import pwconfig,pwargs\n",
    "parsl.load(pwconfig)\n",
    "\n",
    "print(\"pwconfig loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def run_ocean_parcels(stdout='ocean_parcels.stdout', \n",
    "                      stderr='ocean_parcels.stderr', inputs=[], outputs=[]):\n",
    "    \n",
    "    import os\n",
    "    run_script = os.path.basename(inputs[0])\n",
    "    container_run = os.path.basename(inputs[1])\n",
    "    out_file = os.path.splitext(os.path.basename(inputs[0]))[0]+'.gif'\n",
    "    out_dir = os.path.basename(outputs[0])\n",
    "    \n",
    "    run_command = \"/bin/bash \" + container_run + \" python \" + run_script\n",
    "    \n",
    "    # The text here is interpreted by Python (hence the %s string substitution\n",
    "    # using strings in the tuple at the end of the long text string) and then\n",
    "    # run as a bash app.\n",
    "    return '''\n",
    "        %s\n",
    "        outdir=%s\n",
    "        outfile=%s\n",
    "        mkdir -p $outdir\n",
    "        mv movie.gif $outdir/$outfile\n",
    "    ''' % (run_command,out_dir,out_file)\n",
    "\n",
    "@bash_app\n",
    "def get_date(stdout='getdate.stdout',\n",
    "            stderr='getdate.stderr', inputs=[], outputs=[]):\n",
    "    \n",
    "    import os\n",
    "    out_file = os.path.splitext(os.path.basename(inputs[0]))[0]+'.log'\n",
    "    out_dir = os.path.basename(outputs[0])\n",
    "    \n",
    "    run_command = \"date > \" + out_dir + \"/\" + out_file\n",
    "    \n",
    "    return '''\n",
    "        mkdir -p %s\n",
    "        %s\n",
    "    ''' % (out_dir, run_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_TESTING = False\n",
    "if LOCAL_TESTING:\n",
    "    import argparse\n",
    "    pwargs = argparse.Namespace()\n",
    "    pwargs.out_dir = '/pw/storage/test-outputs'\n",
    "    pwargs.run_files = '/pw/workflows/ocean_parcels_demo/test_ocean_parcels_1.py---/pw/workflows/ocean_parcels_demo/test_ocean_parcels_2.py'\n",
    "    wrapper = Path(\"/pw/workflows/ocean_parcels_demo/ocean-parcels/wrap_docker_oceanparcels.sh\")\n",
    "else:\n",
    "    if pwargs.container_type == \"True\":\n",
    "        wrapper = Path(\"/pw/workflows/ocean_parcels_demo/ocean-parcels/wrap_docker_oceanparcels.sh\")\n",
    "    else:\n",
    "        wrapper = Path(\"/pw/workflows/ocean_parcels_demo/ocean-parcels/wrap_singularity_oceanparcels.sh\")\n",
    "        \n",
    "run_files = pwargs.run_files.split('---')\n",
    "\n",
    "runs=[]\n",
    "for run_file_name in run_files:\n",
    "    \n",
    "    run_file = Path(run_file_name)\n",
    "    out_dir = Path(pwargs.out_dir)\n",
    "    \n",
    "    r = run_ocean_parcels(inputs=[run_file,wrapper], \n",
    "                          outputs=[out_dir])\n",
    "    \n",
    "    runs.append(r)\n",
    "\n",
    "print(\"Running\",len(runs),\"OceanParcels executions...\")\n",
    "[r.result() for r in runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
