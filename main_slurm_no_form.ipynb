{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial showing how to create Parcels in Agulhas animated gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief tutorial shows how to recreate the [animated gif](http://oceanparcels.org/animated-gifs/globcurrent_fullyseeded.gif) showing particles in the Agulhas region south of Africa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing the relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Compiled ParcelsRandom ==> /tmp/parcels-40642/libparcels_random_bc2987a5-ca79-4f71-8595-ecdef368b180.so\n"
     ]
    }
   ],
   "source": [
    "from parcels import FieldSet, ParticleSet, JITParticle, AdvectionRK4, ErrorCode\n",
    "from datetime import timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the Globcurrent fields from the `GlobCurrent_example_data` directory (note that unlike in the main Parcels tutorial we don't use a dictionary for the filenames here; as they are the same for all variables, we don't need to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Casting lon data to np.float32\n",
      "WARNING: Casting lat data to np.float32\n",
      "WARNING: Casting depth data to np.float32\n"
     ]
    }
   ],
   "source": [
    "filenames = \"/contrib/Stefan.Gary/jupyterlab/parcels_examples/GlobCurrent_example_data/20*.nc\"\n",
    "variables = {'U': 'eastward_eulerian_current_velocity',\n",
    "             'V': 'northward_eulerian_current_velocity'}\n",
    "dimensions = {'lat': 'lat',\n",
    "              'lon': 'lon',\n",
    "              'time': 'time'}\n",
    "fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create vectors of Longitude and Latitude starting locations on a regular mesh, and use these to initialise a `ParticleSet` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons, lats = np.meshgrid(range(15, 35), range(-40, -30))\n",
    "pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lons, lat=lats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to advect the particles. However, the Globcurrent data that we loaded in is only for a limited, regional domain and particles might be able to leave this domain. We therefore need to tell Parcels that particles that leave the domain need to be deleted. We do that using a `Recovery Kernel`, which will be invoked when a particle encounters an `ErrorOutOfBounds` error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can advect the particles. Note that we do this inside a `for`-loop, so we can save a plot every six hours (which is the value of `runtime`). See the [plotting tutorial](http://nbviewer.jupyter.org/github/OceanParcels/parcels/blob/master/examples/tutorial_plotting.ipynb) for more information on the `pset.show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particles00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Casting field data to np.float32\n",
      "/contrib/Stefan.Gary/miniconda3/envs/oceanparcels/lib/python3.8/site-packages/parcels/plotting.py:191: RuntimeWarning: invalid value encountered in true_divide\n",
      "  u = np.where(speed > 0., data[0]/speed, 0)\n",
      "/contrib/Stefan.Gary/miniconda3/envs/oceanparcels/lib/python3.8/site-packages/parcels/plotting.py:192: RuntimeWarning: invalid value encountered in true_divide\n",
      "  v = np.where(speed > 0., data[1]/speed, 0)\n",
      "INFO: Plot saved to particles00.png\n",
      "INFO: Compiled JITParticleAdvectionRK4 ==> /tmp/parcels-40642/11dcc244d2eeaaa77d5025d504e0b49f_0.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particles01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Plot saved to particles01.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particles02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Plot saved to particles02.png\n"
     ]
    }
   ],
   "source": [
    "# Inputs: pset, DeleteParticle\n",
    "# Outputs: list of files (each file name is in savefile)\n",
    "for cnt in range(3):\n",
    "    # Set filename for output plot\n",
    "    output_image = 'particles'+str(cnt).zfill(2)\n",
    "    print(output_image)\n",
    "    \n",
    "    # First plot the particles\n",
    "    pset.show(savefile=output_image, field='vector', land=True, vmax=2.0)\n",
    "\n",
    "    # Then advect the particles for 6 hours\n",
    "    pset.execute(AdvectionRK4,\n",
    "                 runtime=timedelta(hours=6),  # runtime controls the interval of the plots\n",
    "                 dt=timedelta(minutes=5),\n",
    "                 recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle})  # the recovery kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now has created 3 plots. Note that the original animated gif contained 20 plots, but to keep running of this notebook fast we have reduced the number here. Of course, it is trivial to increase the number of plots by changing the value in the `range()` in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, you can use [ImageMagick](http://www.imagemagick.org/script/index.php) or an online tool to stitch these individual plots together in an animated gif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the above in parallel on remote, custom workers\n",
    "\n",
    "+ Run OceanParcels in 4 separate experiments at the same time.\n",
    "+ Each experiment is defined by a Python/OceanParsels script.\n",
    "+ One experiment per (customizable) worker node.\n",
    "+ Each instance of OceanParcels is run via a Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwconfig loaded\n"
     ]
    }
   ],
   "source": [
    "import parsl\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.data_provider.files import File\n",
    "from path import Path\n",
    "import os\n",
    "#from parsl.configs.local_threads import config\n",
    "#from parslpw import pwconfig,pwargs\n",
    "\n",
    "prefix = ''\n",
    "from parsl.config import Config\n",
    "from parsl.executors.threads import ThreadPoolExecutor\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import SlurmProvider\n",
    "config = Config(\n",
    "    executors=[\n",
    "        HighThroughputExecutor(\n",
    "            label='slurm',\n",
    "            worker_debug=False,\n",
    "            cores_per_worker=int(1),\n",
    "            working_dir = '/tmp/pworks/',\n",
    "            worker_logdir_root = os. getcwd() + '/parsllogs',\n",
    "            provider = SlurmProvider(\n",
    "                partition = 'compute',\n",
    "                nodes_per_block = 1,\n",
    "                min_blocks = int(0),\n",
    "                max_blocks = int(10),\n",
    "                parallelism = 1 # Was 0.80 ### # FIXME?,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "parsl.load(config)\n",
    "\n",
    "print(\"pwconfig loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def run_ocean_parcels(stdout='ocean_parcels.stdout', \n",
    "                      stderr='ocean_parcels.stderr', inputs=[], outputs=[]):\n",
    "    \n",
    "    import os\n",
    "    run_script = os.path.basename(inputs[0])\n",
    "    container_run = os.path.basename(inputs[1])\n",
    "    out_file = os.path.splitext(os.path.basename(inputs[0]))[0]+'.gif'\n",
    "    out_dir = os.path.basename(outputs[0])\n",
    "    \n",
    "    run_command = \"/bin/bash \" + container_run + \" \" + run_script\n",
    "    \n",
    "    # The text here is interpreted by Python (hence the %s string substitution\n",
    "    # using strings in the tuple at the end of the long text string) and then\n",
    "    # run as a bash app.\n",
    "\n",
    "    return '''\n",
    "        outdir=%s\n",
    "        outfile=%s\n",
    "        mkdir -p $outdir\n",
    "        cd $outdir\n",
    "        cp ../%s ./\n",
    "        cp -sv ../%s ./\n",
    "        %s\n",
    "        mv movie.gif $outdir/$outfile\n",
    "    ''' % (out_dir,out_file,run_script,container_run,run_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 4 OceanParcels executions...\n"
     ]
    }
   ],
   "source": [
    "LOCAL_TESTING = True\n",
    "if LOCAL_TESTING:\n",
    "    import argparse\n",
    "    pwargs = argparse.Namespace()\n",
    "    pwargs.out_dir = '/contrib/Stefan.Gary/jupyterlab/ocean-parcels/test-outputs'\n",
    "    pwargs.run_files = '/contrib/Stefan.Gary/jupyterlab/ocean-parcels/test_ocean_parcels_1.py---/contrib/Stefan.Gary/jupyterlab/ocean-parcels/test_ocean_parcels_2.py---/contrib/Stefan.Gary/jupyterlab/ocean-parcels/test_ocean_parcels_3.py---/contrib/Stefan.Gary/jupyterlab/ocean-parcels/test_ocean_parcels_4.py'\n",
    "\n",
    "wrapper = Path(\"/contrib/Stefan.Gary/jupyterlab/ocean-parcels/wrap_singularity_oceanparcels_slurm.sh\")\n",
    "run_files = pwargs.run_files.split('---')\n",
    "\n",
    "runs=[]\n",
    "for run_file_name in run_files:\n",
    "    \n",
    "    # An Ocean Parcels script\n",
    "    run_file = Path(run_file_name)\n",
    "    \n",
    "    # Output directory based on script name\n",
    "    out_dir = Path(os.path.splitext(os.path.basename(run_file_name))[0])\n",
    "    \n",
    "    # Run the containers in parallel in separate directories\n",
    "    r = run_ocean_parcels(inputs=[run_file,wrapper], \n",
    "                          outputs=[out_dir])\n",
    "    \n",
    "    runs.append(r)\n",
    "\n",
    "print(\"Running\",len(runs),\"OceanParcels executions...\")\n",
    "[r.result() for r in runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
